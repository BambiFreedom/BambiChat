{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jof2u6xTSK7t"
      },
      "outputs": [],
      "source": [
        "#### This needs to be run first, and only once.\n",
        "\n",
        "!pip install transformers optimum\n",
        "!pip3 install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/  # Use cu117 if on CUDA 11.7\n",
        "!pip install optimum"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### This needs to be second and only once.\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
      ],
      "metadata": {
        "id": "lXRnpxh6SryV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### This needs to be run third, and any time a model is changed / updated, it needs to be run again.\n",
        "#### If you don't change the model during runtime, you don't need to run this a second time.\n",
        "#### This model is available for free from huggingface.  If you want to try a different model,\n",
        "#### feel free to checkk out huggingface.  It's free.\n",
        "\n",
        "model_name_or_path = \"TheBloke/dolphin-2.1-mistral-7B-GPTQ\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
        "                                             device_map=\"auto\",\n",
        "                                             trust_remote_code=False,\n",
        "                                             revision=\"main\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)"
      ],
      "metadata": {
        "id": "_DrrfiB8SzDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### This can be run as many times as needed.  Each query will be a new response.\n",
        "#### This is a very VERY basic function, with no memory.  Each new query will erase\n",
        "#### the last.  Writing to disk in python is easy to learn, but for simplicity sake we\n",
        "#### have made this code as bare as possible.  Feel free to improve it for whatever you\n",
        "#### need it to do.  If you have never written anything in python before (or any other language)\n",
        "#### then W3School is a very good site to start with.  It is free and beginner friendly.\n",
        "#### We have no affiliation with W3School.  It's just the best of all the sites.\n",
        "\n",
        "\n",
        "# system_message is the personality / context.\n",
        "system_message =\"you are a dog.\"\n",
        "# prompt is what you want to produce.\n",
        "prompt = \"What do you think of steak?\"\n",
        "####\n",
        "# Nothing needs to be changed here.  If you're brave, you can break it in all sorts of interesting ways.\n",
        "# But if you just want this to do what it does, stick to leaving everything alone except the 'system_message' and 'prompt'\n",
        "# variables above.\n",
        "prompt_template=f'''<|im_start|>system\n",
        "{system_message}<|im_end|>\n",
        "<|im_start|>user\n",
        "{prompt}<|im_end|>\n",
        "<|im_start|>assistant\n",
        "'''\n",
        "####\n",
        "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
        "output = model.generate(inputs=input_ids, temperature=1, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=45000)\n",
        "x = (tokenizer.decode(output[0]))\n",
        "cleanedup = str(x.split(\"assistant\")[1].strip(\"<|im_end|>\").strip())\n",
        "print(cleanedup)"
      ],
      "metadata": {
        "id": "xI_UAczTTgFI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}